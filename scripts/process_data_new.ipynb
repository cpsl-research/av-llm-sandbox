{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    # KEEP IT PRIVATE!\n",
    "    api_key=\"sk-***\",\n",
    ")\n",
    "max_retries = 5\n",
    "retry_delay = 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'dataset_train.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    data = file.read()\n",
    "index_dict = json.loads(data)\n",
    "dataset = index_dict['dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def flatten_frames(src_dataset):\n",
    "    \"\"\"\n",
    "    Return a list of dicts — one per frame — with keys:\n",
    "        frame_id, meta_action (str), waypoints_2d (str), image_paths (dict)\n",
    "    Scene/agent IDs are discarded.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for scene in src_dataset.values():\n",
    "        for agent in scene.values():\n",
    "            for fid, frame in agent.items():\n",
    "                # ---------- 1) meta_action  ----------\n",
    "                lat_vals = []\n",
    "                lon_vals = []\n",
    "                \n",
    "                for rec in frame[\"meta_actions\"].values():\n",
    "                    if rec is not None:\n",
    "                        lat, lon = rec.get(\"lateral\"), rec.get(\"longitudinal\")\n",
    "                    lat_vals.append(lat)\n",
    "                    lon_vals.append(lon)\n",
    "                '''\n",
    "                meta_action = frame[\"meta_actions\"]['dt_2.00']\n",
    "                if meta_action is not None:\n",
    "                    lat, lon = meta_action.get(\"lateral\"), meta_action.get(\"longitudinal\")\n",
    "                    meta_action_str = str([lat, lon])\n",
    "                else:\n",
    "                    meta_action_str = str([None, None])\n",
    "                '''\n",
    "                meta_action_str = [lat_vals, lon_vals]\n",
    "                #majority_lat = Counter(lat_vals).most_common(1)[0][0] if lat_vals else None\n",
    "                #majority_lon = Counter(lon_vals).most_common(1)[0][0] if lon_vals else None\n",
    "                #meta_action_str = str([majority_lat, majority_lon])\n",
    "\n",
    "                # ---------- 2) waypoints_2d  ----------\n",
    "                \n",
    "                # sort by the integer part of 'dt_X'\n",
    "                tuples = []\n",
    "                for wp in frame.get(\"waypoints_3d\", {}).values():\n",
    "                    if wp and len(wp) >= 3:\n",
    "                        x, _, z = wp\n",
    "                        tuples.append((round(x, 1), round(z, 1)))\n",
    "                waypoints_str = str(tuples)\n",
    "                \n",
    "                \n",
    "                speeds = [st[\"speed\"] for st in frame[\"agent_state\"].values()\n",
    "                          if \"speed\" in st]\n",
    "                speed_val = round(speeds[0],1) if speeds else None\n",
    "                # ---------- 3) pack result ----------\n",
    "                out.append({\n",
    "                    \"frame_id\":      fid,\n",
    "                    \"image_paths\":   frame[\"image_paths\"],  # untouched\n",
    "                    \"meta_action\":   meta_action_str,\n",
    "                    \"waypoints_2d\":  waypoints_str,\n",
    "                    \"speed\": speed_val,\n",
    "                })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame_id': 'frame_3',\n",
       " 'image_paths': {'CAM_BACK': '../data/nuscenes-full/samples/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886384137525.jpg',\n",
       "  'CAM_BACK_LEFT': '../data/nuscenes-full/samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886384147423.jpg',\n",
       "  'CAM_BACK_RIGHT': '../data/nuscenes-full/samples/CAM_BACK_RIGHT/n015-2018-07-18-11-50-34+0800__CAM_BACK_RIGHT__1531886384127893.jpg',\n",
       "  'CAM_FRONT': '../data/nuscenes-full/samples/CAM_FRONT/n015-2018-07-18-11-50-34+0800__CAM_FRONT__1531886384112473.jpg',\n",
       "  'CAM_FRONT_LEFT': '../data/nuscenes-full/samples/CAM_FRONT_LEFT/n015-2018-07-18-11-50-34+0800__CAM_FRONT_LEFT__1531886384104844.jpg',\n",
       "  'CAM_FRONT_RIGHT': '../data/nuscenes-full/samples/CAM_FRONT_RIGHT/n015-2018-07-18-11-50-34+0800__CAM_FRONT_RIGHT__1531886384120339.jpg'},\n",
       " 'meta_action': [['VEER_RIGHT', 'VEER_RIGHT', 'STRAIGHT'],\n",
       "  ['MAINTAIN', 'ACCELERATE', 'ACCELERATE']],\n",
       " 'waypoints_2d': '[(0.2, 2.8), (0.5, 7.3), (1.0, 11.9), (1.5, 16.5), (1.9, 21.0), (2.2, 26.2)]',\n",
       " 'speed': 8.7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 999\n",
    "outset = flatten_frames(dataset)\n",
    "outset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(path_to_image: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads an image file from disk and returns a base64-encoded string (JPEG).\n",
    "    \"\"\"\n",
    "    with open(path_to_image, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "def build_autonomous_driving_prompt(camera_info_dict, use_base64=False):\n",
    "    \"\"\"\n",
    "    Build a concise prompt for an LLM to perform step‑by‑step scene reasoning\n",
    "    from six surround‑view images.  The model must output three numbered\n",
    "    sections—Perception, Prediction, Road—without prescribing any action.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- System role ---------------------------------------------------\n",
    "    system_prompt = \"\"\"\n",
    "You are an autonomous‑driving vision analyst.\n",
    "Think step‑by‑step and output ONLY the three sections below.\n",
    "Do NOT suggest steering or speed commands.\n",
    "\"\"\".strip()\n",
    "\n",
    "    # -------- User instructions & demo --------------------------------------\n",
    "    user_prompt = \"\"\"\n",
    "### Task\n",
    "From the six camera views, give a brief *situation report*:\n",
    "\n",
    "1) **Detected Objects** – main vehicles, pedestrains, traffic lights, and road signs, etc., their state, lane/relative position, ≈distance (m).  \n",
    "2) **Predicted Movements** – likely next motion for each key object.  \n",
    "3) **Road Condition Ahead** – geometry in front of the ego car (e.g., “straight & clear”, “tight left‑hand curve”).\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Camera Views (sample):\n",
    "• front‑left – parked cars at curb  \n",
    "• front       – blue sedan 30 m ahead, braking  \n",
    "• front‑right – clear sidewalk  \n",
    "• back‑left   – black SUV closing in left lane  \n",
    "• back        – clear  \n",
    "• back‑right  – cyclist 20 m behind\n",
    "\n",
    "**Model Output**\n",
    "1) Blue sedan ahead braking ~30 m on the front view; black SUV left‑rear closing fast; cyclist right‑rear steady ~20 m.  \n",
    "2) Sedan will slow further; SUV may merge right; cyclist continues straight.  \n",
    "3) Road ahead straight and unobstructed.\n",
    "\n",
    "---\n",
    "\n",
    "Now analyse the new scene:\n",
    "\n",
    "\"\"\".lstrip()\n",
    "\n",
    "    # -------- Assemble messages --------------------------------------------\n",
    "    system_content    = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content      = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"1)\"}]   # model’s first token cue\n",
    "\n",
    "    # Add camera views\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\"})\n",
    "    if use_base64:\n",
    "        for view, path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            encoded = encode_image(path)                 # assume helper exists\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded}\"}\n",
    "            })\n",
    "    else:\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}: {desc}\"})\n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_reasoning_and_action(client, camera_dict, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_autonomous_driving_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAM_BACK': '../data/nuscenes-full/samples/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886337887525.jpg',\n",
       " 'CAM_BACK_LEFT': '../data/nuscenes-full/samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886337898056.jpg',\n",
       " 'CAM_BACK_RIGHT': '../data/nuscenes-full/samples/CAM_BACK_RIGHT/n015-2018-07-18-11-50-34+0800__CAM_BACK_RIGHT__1531886337878549.jpg',\n",
       " 'CAM_FRONT': '../data/nuscenes-full/samples/CAM_FRONT/n015-2018-07-18-11-50-34+0800__CAM_FRONT__1531886337862460.jpg',\n",
       " 'CAM_FRONT_LEFT': '../data/nuscenes-full/samples/CAM_FRONT_LEFT/n015-2018-07-18-11-50-34+0800__CAM_FRONT_LEFT__1531886337854851.jpg',\n",
       " 'CAM_FRONT_RIGHT': '../data/nuscenes-full/samples/CAM_FRONT_RIGHT/n015-2018-07-18-11-50-34+0800__CAM_FRONT_RIGHT__1531886337870339.jpg'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = outset[index]['image_paths']\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) **Detected Objects**: \\n   - **Front**: A white bus is approximately 20 meters ahead in the same lane.\\n   - **Back**: A red vehicle is following at a moderate distance, roughly 30 meters.\\n   - **Front-Left, Front-Right, Back-Left, Back-Right**: No vehicles or pedestrians detected, clear views with grass and trees.\\n\\n2) **Predicted Movements**:\\n   - The bus ahead may slow down further or continue straight. No immediate braking action detected.\\n   - The red vehicle behind appears to be maintaining its distance, likely to continue straight.\\n\\n3) **Road Condition Ahead**:\\n   - The road is straight with slight curves, unobstructed, bordered by grass and trees.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate_reasoning_and_action(client, image_path, use_base64=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_verify_prompt(reasoning_context: str, speed: float):\n",
    "    \"\"\"\n",
    "    Construct a concise, high‑signal prompt for an LLM that returns a driving\n",
    "    meta‑action pair and a confidence score from 0‑5.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are an autonomous‑driving assistant.  \n",
    "    Input: reasoning context + ego speed.  \n",
    "    Task: decide what the ego vehicle should do from the lateral and longitudinal aspects.\n",
    "\n",
    "    Output format (no extra text):\n",
    "    (['<LATERAL>', '<LONGITUDINAL>'], <CONFIDENCE>)    # confidence ∈ 0‑5\n",
    "\n",
    "    Allowed meta‑actions\n",
    "      • Lateral:   VEER_LEFT | VEER_RIGHT | CHANGE_LANE_LEFT | CHANGE_LANE_RIGHT\n",
    "                  STRAIGHT  | TURN_LEFT  | TURN_RIGHT\n",
    "      • Longitudinal: ACCELERATE | MAINTAIN | DECELERATE | REVERSE\n",
    "\n",
    "    Decision rules\n",
    "      1. Avoid collisions; keep safe gaps.\n",
    "      2. Stay on drivable surface.\n",
    "      3. keep reasonable speed when road is clear\n",
    "      4. Turning with low speed and deceleration.\n",
    "\n",
    "    Considerations(IMPORTANT)\n",
    "    • Lateral:\n",
    "      1. Check roadway geometry first. If the main lane curves ahead, select the action that\n",
    "        follows the curve (never output STRAIGHT in this case).\n",
    "      2. Then account for pedestrians, vehicles, or other obstacles and steer to avoid\n",
    "        any potential collision.\n",
    "\n",
    "    • Longitudinal:\n",
    "      1. Begin with the current speed.\n",
    "      2. Decide on a change:\n",
    "        - If the vehicle is moving too slowly for conditions, ACCELERATE.  \n",
    "        - If it’s too fast or needs extra margin, DECELERATE. \n",
    "        - If ego is turning left or right, DECELERATE.\n",
    "        - Otherwise, MAINTAIN the present speed.\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    # Assemble chat messages\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt.strip()}]\n",
    "    user_content = [\n",
    "        {\"type\": \"text\", \"text\": f\"Reasoning context:\\n{reasoning_context}\"},\n",
    "        {\"type\": \"text\", \"text\": f\"Ego speed: {speed}m/s\"},\n",
    "    ]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Meta‑action and confidence:\"}]\n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_final_action(client, reasoning_context, speed):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_verify_prompt(\n",
    "        reasoning_context,\n",
    "        speed\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(['STRAIGHT', 'DECELERATE'], 4)\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed = outset[index]['speed']\n",
    "print(speed)\n",
    "action = generate_final_action(client, output, speed)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refine_prompt(\n",
    "    camera_info_dict,\n",
    "    reasoning_context,\n",
    "    use_base64=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant specialized in \n",
    "        concise reasoning about camera images to determine the correct driving action.\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Six camera images (front-left, front, front-right, back-left, back, back-right).\n",
    "    2) A current chain-of-thought reasoning.\n",
    "\n",
    "    Goal: Produce a shorter, more concise version of the reasoning that only includes details \n",
    "    necessary for deriving the final driving action. Remove unnecessary analysis, extraneous \n",
    "    tangents, or repeated points. Rephrase any sentences to be more succinct while preserving \n",
    "    meaning.\n",
    "\n",
    "    Instructions:\n",
    "    1. Review the camera images and the current reasoning.\n",
    "    2. Delete or omit irrelevant details that do not influence the final driving action.\n",
    "    3. Rephrase what's left so it's concise but still logically consistent.\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Concise reasoning:\"}]\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\\n\"})\n",
    "    if use_base64:\n",
    "        # Insert base64 data (GPT-3.5/4 standard models typically cannot decode, but let's show it anyway)\n",
    "        for view, image_path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })                    \n",
    "    else:\n",
    "        # Insert textual descriptions\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            user_content.append({\"type\": \"text\", \"text\": desc})\n",
    "\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Current reasoning chain:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": reasoning_context})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_concise_reasoning(client, camera_dict, reasoning_context, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_refine_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        reasoning_context=reasoning_context,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The road ahead is clear, with a gentle left curve. There are construction barriers and a worker on the left, but they do not obstruct the lane. A pedestrian on the right sidewalk is moving away. No vehicles or obstacles are present in the path. Proceed forward with caution near the construction area.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concise_reasoning = generate_concise_reasoning(client, image_path, output, use_base64=True)\n",
    "concise_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TURN_LEFT', 'MAINTAIN']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "action = generate_final_action(client, concise_reasoning, speed)\n",
    "action = ast.literal_eval(action)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STRAIGHT', 'DECEL'} {'STRAIGHT', 'DECEL'}\n"
     ]
    }
   ],
   "source": [
    "#action = ast.literal_eval(action)[0]\n",
    "print(set(action),set(ast.literal_eval(outset[index]['meta_action'])))\n",
    "assert set(action) == set(ast.literal_eval(outset[index]['meta_action']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f078dc2805cd4a8a933983e8fc57d2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
