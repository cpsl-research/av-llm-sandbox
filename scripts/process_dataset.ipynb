{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    # KEEP IT PRIVATE!\n",
    "    api_key=\"sk-***\",\n",
    ")\n",
    "max_retries = 5\n",
    "retry_delay = 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'dataset_train.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    data = file.read()\n",
    "index_dict = json.loads(data)\n",
    "dataset = index_dict['dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def flatten_frames(src_dataset):\n",
    "    \"\"\"\n",
    "    Return a list of dicts — one per frame — with keys:\n",
    "        frame_id, meta_action (str), waypoints_2d (str), image_paths (dict)\n",
    "    Scene/agent IDs are discarded.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for scene in src_dataset.values():\n",
    "        for agent in scene.values():\n",
    "            for fid, frame in agent.items():\n",
    "                # ---------- 1) meta_action  ----------\n",
    "                lat_vals = []\n",
    "                lon_vals = []\n",
    "                for rec in frame[\"meta_actions\"].values():\n",
    "                    if rec is not None:\n",
    "                        lat, lon = rec.get(\"lateral\"), rec.get(\"longitudinal\")\n",
    "                    lat_vals.append(lat)\n",
    "                    lon_vals.append(lon)\n",
    "                    \n",
    "\n",
    "                majority_lat = Counter(lat_vals).most_common(1)[0][0] if lat_vals else None\n",
    "                majority_lon = Counter(lon_vals).most_common(1)[0][0] if lon_vals else None\n",
    "                meta_action_str = str([majority_lat, majority_lon])\n",
    "\n",
    "                # ---------- 2) waypoints_2d  ----------\n",
    "                tuples = []\n",
    "                # sort by the integer part of 'dt_X'\n",
    "                for k in sorted(frame[\"waypoints_3d\"],\n",
    "                                key=lambda s: int(s.split('_')[1])):\n",
    "                    if frame[\"waypoints_3d\"][k] is not None:\n",
    "                        x, _, z = frame[\"waypoints_3d\"][k]\n",
    "                        tuples.append((round(x, 1), round(z, 1)))\n",
    "                waypoints_str = str(tuples)\n",
    "                \n",
    "                speeds = [st[\"speed\"] for st in frame[\"agent_state\"].values()\n",
    "                          if \"speed\" in st]\n",
    "                speed_val = round(speeds[0],1) if speeds else None\n",
    "                # ---------- 3) pack result ----------\n",
    "                out.append({\n",
    "                    \"frame_id\":      fid,\n",
    "                    \"image_paths\":   frame[\"image_paths\"],  # untouched\n",
    "                    \"meta_action\":   meta_action_str,\n",
    "                    \"waypoints_2d\":  waypoints_str,\n",
    "                    \"speed\": speed_val,\n",
    "                })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outset = flatten_frames(dataset)\n",
    "outset[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cam_front_objects(data_dict):\n",
    "    \"\"\"\n",
    "    For each scene in data_dict, collect only the objects whose keys\n",
    "    contain 'CAM_FRONT'. Return a list of dictionaries, one per scene:\n",
    "    \n",
    "    {\n",
    "      'scene_id': ...,\n",
    "      'scene_description': ...,\n",
    "      'objects': [\n",
    "          {\n",
    "            'object_key': ...,\n",
    "            'Category': ...,\n",
    "            'Status': ...,\n",
    "            'Visual_description': ...,\n",
    "            '2d_bbox': ...\n",
    "          },\n",
    "          ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    scenes_output = []\n",
    "\n",
    "    for scene_id, scene_data in data_dict.items():\n",
    "        \n",
    "        scene_desc = scene_data.get(\"scene_description\", \"N/A\")\n",
    "        key_frames = scene_data.get(\"key_frames\", {})\n",
    "        # We'll gather only front-camera objects in this list\n",
    "        for frame_id, frame_data in key_frames.items():\n",
    "            key_object_infos = frame_data.get(\"key_object_infos\", {})\n",
    "            image_paths = frame_data.get(\"image_paths\", {})\n",
    "            cam_front_objects = {}\n",
    "            cam_front_objects['objects'] = []\n",
    "            cam_front_objects['image_path'] = image_paths\n",
    "            for obj_key, obj_info in key_object_infos.items():\n",
    "                \n",
    "                category = obj_info.get(\"Category\", \"Unknown\")\n",
    "                status = obj_info.get(\"Status\", \"Unknown\")\n",
    "                visual_desc = obj_info.get(\"Visual_description\", \"\")\n",
    "                bbox = obj_info.get(\"2d_bbox\", [])\n",
    "\n",
    "                cam_front_objects[\"objects\"].append({\n",
    "                    \"object_key\": obj_key,\n",
    "                    \"Category\": category,\n",
    "                    \"Status\": status,\n",
    "                    \"Visual_description\": visual_desc,\n",
    "                    \"2d_bbox\": bbox\n",
    "                    # Note: we are NOT including 'frame_id' here\n",
    "                })\n",
    "            scene_entry = {\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_description\": scene_desc,\n",
    "                \"frame_id\": frame_id,\n",
    "                \"objects\": cam_front_objects\n",
    "            }\n",
    "            \n",
    "            scenes_output.append(scene_entry)\n",
    "        \n",
    "    \n",
    "    return scenes_output\n",
    "\n",
    "results = extract_cam_front_objects(index_dict)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(path_to_image: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads an image file from disk and returns a base64-encoded string (JPEG).\n",
    "    \"\"\"\n",
    "    with open(path_to_image, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "def build_autonomous_driving_prompt(\n",
    "    camera_info_dict,\n",
    "    use_base64=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant with expertise in scene understanding, object detection, and action planning.\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Six camera images (front-left, front, front-right, back-left, back, back-right).\n",
    "    2) A demonstration example showing how you should reason step-by-step and provide a final action.\n",
    "\n",
    "    Please follow this structure:\n",
    "    1) Summarize the detected objects and their statuses from the images.\n",
    "    2) Predict the future movement or intent of key objects.\n",
    "    3) Propose reasoning on the above content and give some corresponding potential safe and correct driving actions.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Demonstration Example\n",
    "\n",
    "    **Camera Views (Example)**:\n",
    "    1) front-left: [an image showing parked cars on the curb, no pedestrians].\n",
    "    2) front: [an image showing a blue sedan ~30m ahead, slight braking.\n",
    "    3) front-right: [an image showing clear sidewalk, no immediate obstacles].\n",
    "    4) back-left: [an image showing a black SUV approaching quickly in the left lane].\n",
    "    5) back: Clear, [an image showing no vehicle behind in the same lane].\n",
    "    6) back-right: [an image showing a bicycle rider moving in the right lane behind].\n",
    "\n",
    "    **Sample Step-by-Step Reasoning (Example)**:\n",
    "    1) Detected Objects & Status:\n",
    "    - Blue sedan ahead is braking slightly.\n",
    "    - Black SUV is behind in the adjacent lane, accelerating.\n",
    "    - Bicycle behind to the right, stable speed.\n",
    "    2) Future Movement Prediction:\n",
    "    - The sedan may slow further or maintain a slower speed.\n",
    "    - The SUV may attempt to pass or merge.\n",
    "    - The bicycle will continue along the right lane.\n",
    "    3) Action Planning:\n",
    "    - Since the sedan is slowing, be prepared to reduce speed.\n",
    "    - The SUV might merge, so keep safe distance and monitor left mirror.\n",
    "    - Maintain lane position and reduce speed to maintain a safe following distance.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Now Your Turn\n",
    "\n",
    "    Below is the new scenario for which we need the same type of reasoning. Please produce a step-by-step reasoning content including perception, prediction and planning, following the style shown in the example.\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Step-by-Step reasoning:\"}]\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\\n\"})\n",
    "    if use_base64:\n",
    "        # Insert base64 data (GPT-3.5/4 standard models typically cannot decode, but let's show it anyway)\n",
    "        for view, image_path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })                    \n",
    "    else:\n",
    "        # Insert textual descriptions\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            user_content.append({\"type\": \"text\", \"text\": desc})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_reasoning_and_action(client, camera_dict, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_autonomous_driving_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = str(results[0]['objects']['objects'])\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = results[1]['objects']['image_path']\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_reasoning_and_action(client, image_path, use_base64=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_multiple_choice(client, question, answer):\n",
    "    \"\"\"\n",
    "    Sends a request to GPT to convert a single Q/A into a 5-option multiple-choice question.\n",
    "    Returns a tuple: (revised_question_string, correct_answer_letter).\n",
    "    \"\"\"\n",
    "\n",
    "    # You can fine-tune this system prompt if desired\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant that rewrites autonomous driving Q/A into multiple-choice format. \"\n",
    "        \"You must produce exactly one correct option that matches the original answer, \"\n",
    "        \"and four distractors that are plausible but incorrect. \"\n",
    "        \"Return output as a Python tuple of two elements: \"\n",
    "        \"(\\\"Revised MC question\\\", \\\"CorrectAnswerLetter\\\"). \"\n",
    "        \"Only one letter from A,B,C,D,E should be correct.\"\n",
    "    )\n",
    "\n",
    "    # User message with the raw Q/A. We instruct GPT how to format\n",
    "    user_message = f\"\"\"\n",
    "        Original Question: {question}\n",
    "        Original Answer: {answer}\n",
    "\n",
    "        Instructions:\n",
    "        1. Convert the Q/A into a multiple-choice question with exactly 5 options (A, B, C, D, E).\n",
    "        2. Only one option should be correct(can be anyone from A to E), reflecting the original answer.\n",
    "        3. Provide 4 other distractor options that are different from the correct one.\n",
    "        4. Format the final output strictly as a Python tuple with two elements:\n",
    "        ( \"<multiline MC question>\", \"<single letter denoting correct answer>\" )\n",
    "        5. The MC question should look like this:\n",
    "\n",
    "        <QUESTION>\n",
    "        A) ...\n",
    "        B) ...\n",
    "        C) ...\n",
    "        D) ...\n",
    "        E) ...\n",
    "\n",
    "        6. The second element of the tuple is the letter of the correct choice, e.g. \"B\".\n",
    "        7. Do not add extra text or explanation outside the tuple.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # or another ChatGPT-compatible model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=400,\n",
    "    )\n",
    "\n",
    "    # The assistant's reply should be a string that looks like: \n",
    "    # ( \"Predict the behavior of the ego vehicle...\\nA) ...\\nB) ...\\nC) ...\\nD) ...\\nE) ...\", \"C\" )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = results[0]['scene_id']\n",
    "frame_id = results[0]['frame_id']\n",
    "QA = index_dict[scene_id]['key_frames'][frame_id]['QA']['behavior'][0]\n",
    "question = QA['Q']\n",
    "answer = QA['A']\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "new_QA = qa_to_multiple_choice(client, question, answer)\n",
    "new_QA = new_QA.replace('```','').replace('python','').replace('\\n','').replace('\\\\n','')\n",
    "new_QA = ast.literal_eval(new_QA)\n",
    "new_Q = new_QA[0]\n",
    "new_A = new_QA[1]\n",
    "new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_verify_prompt(\n",
    "    reasoning_context,\n",
    "    question\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant that uses provided reasoning context\n",
    "        to answer multiple-choice questions. You should pick the single best correct option.\"\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Reasoning context derived from a real driving scenario.\n",
    "    2) A multi-choice question asking about the correct and safe driving action.\n",
    "\n",
    "    Instruction\n",
    "    1) Please analyze the reasoning context carefully, then select the single best answer (A, B, C, D, or E).\n",
    "    2) Only output a single letter\n",
    "\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"The single letter answer is:\"}]\n",
    "    \n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Reasoning context:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": reasoning_context})\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Multi-choice question:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": question})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_final_action(client, reasoning_context, question):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_verify_prompt(\n",
    "        reasoning_context,\n",
    "        question\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=4\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = generate_final_action(client, output, new_Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refine_prompt(\n",
    "    camera_info_dict,\n",
    "    reasoning_context,\n",
    "    use_base64=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant specialized in \n",
    "        concise reasoning about camera images to determine the correct driving action.\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Six camera images (front-left, front, front-right, back-left, back, back-right).\n",
    "    2) A current chain-of-thought reasoning.\n",
    "\n",
    "    Goal: Produce a shorter, more concise version of the reasoning that only includes details \n",
    "    necessary for deriving the final driving action. Remove unnecessary analysis, extraneous \n",
    "    tangents, or repeated points. Rephrase any sentences to be more succinct while preserving \n",
    "    meaning.\n",
    "\n",
    "    Instructions:\n",
    "    1. Review the camera images and the current reasoning.\n",
    "    2. Delete or omit irrelevant details that do not influence the final driving action.\n",
    "    3. Rephrase what's left so it's concise but still logically consistent.\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Concise reasoning:\"}]\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\\n\"})\n",
    "    if use_base64:\n",
    "        # Insert base64 data (GPT-3.5/4 standard models typically cannot decode, but let's show it anyway)\n",
    "        for view, image_path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })                    \n",
    "    else:\n",
    "        # Insert textual descriptions\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            user_content.append({\"type\": \"text\", \"text\": desc})\n",
    "\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Current reasoning chain:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": reasoning_context})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_concise_reasoning(client, camera_dict, reasoning_context, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_refine_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        reasoning_context=reasoning_context,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_reasoning = generate_concise_reasoning(client, image_path, output, use_base64=True)\n",
    "concise_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = generate_final_action(client, concise_reasoning, new_Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert action == new_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "import torch\n",
    "system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant that uses provided reasoning context\n",
    "        to answer multiple-choice questions. You should pick the single best correct option.\n",
    "    '''\n",
    "\n",
    "user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Reasoning context derived from a real driving scenario.\n",
    "    2) A multi-choice question asking about the correct and safe driving action.\n",
    "\n",
    "    Instruction\n",
    "    1) Please analyze the reasoning context carefully, then select the single best answer (A, B, C, D, or E).\n",
    "    2) Only output a single letter\n",
    "    '''\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipe = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "messages = user_prompt+'\\n Reasoning context:'+concise_reasoning+'\\n Multi-choice question:'+new_Q+'\\n The single letter answer is:'\n",
    "\n",
    "out = pipe(messages)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
